\documentclass[a4paper,11pt]{article}

\usepackage[english]{babel}
\usepackage{mathrsfs,amssymb,amsmath,amsthm, enumerate}
\usepackage{verbatim,graphicx,geometry}
%\usetikzlibrary{arrows}
\usepackage[utf8]{inputenc}

\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\makeatletter
\def\@biblabel#1{\hspace*{-\labelsep}}
\makeatother
\geometry{left=1in,right=1
in,top=1in,bottom=1in}
\newdimen\dummy
\dummy=\oddsidemargin
\addtolength{\dummy}{72pt}
\marginparwidth=.5\dummy
\marginparsep=.1\dummy
%\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\usepackage[T1]{fontenc}
\usepackage{beramono}
\usepackage{listings}
\usepackage[usenames,dvipsnames]{xcolor}

%%
%% Julia definition (c) 2014 Jubobs
%%
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do,else,elseif,%
      end,export,false,for,function,immutable,import,importall,if,in,%
      macro,module,otherwise,quote,return,switch,true,try,type,typealias,%
      using,while},%
   sensitive=true,%
   alsoother={\$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%

\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}


\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\plim}{\overset{p}{\longrightarrow}}
\newcommand{\dlim}{\overset{d}{\longrightarrow}}

\begin{document}
\title{Advanced Industrial Organization II \\ Problem Set 2}
\author{Eliot Abrams \and Hyunmin Park \and Alexandre Sollaci}
\date{\today}
\maketitle

\section*{2.1.1}
Say the utility of consumer $i$ when buying product $j$ is given by
\[ u_{i,j} = \alpha_i(w_i - p_j) + \mathbf{x}_j'\mathbf{\beta_i} + \xi_j + \varepsilon_{i,j} \]
and let the utility of the outside good be normalized to zero. If we include a constant here, then the $\mathbf{\beta_i}$ on the constant can be interpreted as the base utility that consumer $i$ gets from consuming an ``inside good" as opposed to the outside good. So say $\mathbf{x}_j^{[1]} = 1$ and we are estimating demand for cars and the outside good is not purchasing a car at all. Then $\beta_i^{[1]}$ will capture the utility that a consumer gets when buying a car, regardless of its characteristics. Note that $\beta_i^{[1]}$ may vary between individuals, since the value of a car to a person probably depends on their own demographic characteristics (this is the approach we follow in our specification of the BLP below). 

\section*{2.1.2}
We estimate the simple logit model via minimization in the MPEC formulation including a constant term. Our estimates for $\alpha$ and $\beta$ are:
\[
\hat\alpha = 1.8185 \quad \mbox{and}  \quad
\hat\beta = \left[\begin{array}{l} 0.9638 \\ 1.4510 \\ 1.7228 \\ 0.3709 \end{array}\right] \]

$\beta$ is ordered such that the first element refers to the coefficient on the constant and the second through fourth refer to the coefficients on variables $x_1, x_2,$ and $x_3$ respectively.

Since we follow the MPEC approach, we also have estimates for the vector of structural errors, $\xi$. It is then straightforward to compute the optimal GMM weighting matrix as

\[\hat W = \frac{1}{J}\left[Z'\hat{D}_{\xi}Z\right]^{-1} \]where $J$ is the number of products$\times$markets and 
\[ \hat{D}_{\xi} = \left(\begin{array}{cccc}
\hat\xi_1^2 & 0 & \ldots & 0 \\
\vdots & \hat\xi_2^2 & \ddots & \vdots \\
0 & 0 & \ldots & \hat\xi_J^2
\end{array} \right) \]


\section*{2.1.3}
Let $K$ be the number of columns in $\mathbf{x}$ (in this particular case, $K = 4$). There are then $1 + K + 3(K+1)$ parameters, since
\begin{itemize}
\item $\alpha$ is a scalar
\item $\beta$ is a $K \times 1$ vector, the same dimension as $\mathbf{x}$
\item $\mathbf{\Pi}$ has two $(K+1) \times 1$ blocks: $\Pi_{\mathrm{inc}}$ and $\Pi_{\mathrm{age}}$, referring to income and age of each consumer. In the way we set up our model, the first $K$ entries of both $\Pi_{\mathrm{inc}}$ and $\Pi_{\mathrm{age}}$ will interact with product characteristics, $\mathbf{x}$, in the market-share equation, while the $K+1^{th}$ entry will interact with price, $\mathbf{p}$
\item $\sqrt{\Sigma}$ has $K\times 1$ parameters, since we assume that $\Sigma$ is a diagonal matrix. As before, in the way we set up our model, $\sigma_{1}, \ldots, \sigma_{K}$ will interact with characteristics while $\sigma_{K+1}$ will interact with prices in the market-share equation
\end{itemize}

We can define the market-share in the model by

\[ s_j = \frac{1}{N}\sum_{n=1}^N \frac{\exp\left(\sum_{k=1}^K
x_{jk}Y^x_{k,n} - Y^p_{n}p_j + \xi_j \right)}{ 1 + \sum_{j = 1}^J\exp\left(\sum_{k=1}^K x_{jk}Y^x_{k,n} - Y^p_{n}p_j + \xi_j \right)} \]
where
\[ Y^x_{k,n} = \beta^{(k)} + \Pi_{\mathrm{inc}}^{(k)}\mathrm{inc}_n + \Pi_{\mathrm{age}}^{(k)}\mathrm{age}_n + \sigma_k \nu_n^k\]
and
\[ Y^p_{n} = \alpha + \Pi_{\mathrm{inc}}^{(K+1)}\mathrm{inc}_n + \Pi_{\mathrm{age}}^{(K+1)}\mathrm{age}_n + \sigma_{K+1}\nu_n^{K+1}. \]

\section*{2.1.4}
We estimate the full model as described in the section below. We fix the optimal weighting matrix to that calculated from the simple logit model and use the estimates of the simple logit model as the starting values. As also described in the section below, the solver, KNITRO, does not converge for our current BLP implementation. The estimates we present here are KNITRO's best guess for the parameters after running for 30 iterations. Note that KNITRO does not throw up any errors or warnings and there is limited improvement in the objective after about 10 iterations. With that disclaimer, our estimates are:
\[
\hat\alpha = -0.8699, \quad
\hat\beta = \left[ \begin{array}{r} 
0.9638 \\ 2.6917 \\ 1.0280 \\ 1.3045
\end{array}\right],
\]
$\hat\beta$ is ordered such that the first element refers to the coefficient on the constant and the second through fourth refer to the coefficients on variables $x_1, x_2,$ and $x_3$ respectively.

\noindent Further,
\[
\hat\Pi_{\mathrm{inc}} = \left[ \begin{array}{r}
1.3283e^{-7} \\ 0.2348 \\ 0.7485 \\ -0.3961 \\ 0.3898
\end{array}\right],
\quad
\hat\Pi_{\mathrm{age}} = \left[ \begin{array}{r}
3.9302e^{-7} \\ 0.0352 \\ -0.0503 \\ 0.0090 \\ 0.1638
\end{array}\right],
\quad
\hat\sigma = \left[ \begin{array}{r}
-4.0364e^{-12} \\ -0.9308 \\ -0.1276 \\ -1.7015 \\ -1.5043
\end{array}\right].
\]
where the estimates above are ordered such that the first element contains the random coefficient that impact the constant, the second though fourth elements contain the coefficients that impact $x_1$, $x_2$ and $x_3$, and the fifth element contains the coefficient that impacts the price.

Comparing the full BLP estimates with the simple logit estimates, we can see that $\hat\beta$ is similar in both approaches. This suggests that, despite the computational effort that computing the full BLP model involves, there might not be huge gains as far as improving the value of the estimates goes. From $\hat\Pi_{\mathrm{inc}}$, $\hat\Pi_{\mathrm{age}}$ and $\hat\sigma$, we can also conclude that the value of consuming an ``inside good'' does not change too much between people with different demographic characteristics. 

We note that the BLP estimation returns a negative value for $\hat\alpha$. This makes little economic sense; it means that a high price is a desirable feature of a product, even when controlling for the product's characteristics (so we can rule out situations in which a high price could be perceived as a signal for high (unobservable) quality, for example). In particular, we would get positive own-price elasticities and a positively sloped demand function. This fact -- that $\hat\alpha$ is negative -- does suggest that our estimate is wrong, either because of some issue in the estimation procedure or because we forced the solver to stop after 30 iterations. 

Unfortunately, due to time constraints, we could not pursue alternative implementations, such as re-doing all our code in Matlab, before the due date of this problem set. A huge ``bright side" of our recent endeavor is that our failure is currently being used to improve the JuMP package and auto-differentiation routines in Julia. Our model will be used to test future versions of JuMP and will be linked to as a coding example for non-linear models from JuMP's website.

\subsection*{A brief note on the estimation procedure}
The program we wrote to estimate the simple logit model was written in Julia and uses the JuMP (Julia for Mathematical Programming) package and the Ipopt solver. We chose Julia to exploit its speed advantage as a compiled language and to gain experience programming in a new language. Further, JuMP facilitates auto-differentiation of the gradient and the constraints--theoretically alleviating the need for the user to painstakingly code the exact Jacobian and Hessians.

The combination of Julia, JuMP, and Ipopt worked well to solve the simple logit model--performing the auto-differentiation and optimization in under 4 seconds.

However, unsurprisingly, the full BLP problem proved to be much more demanding on the auto-differentiation routines. The latest release version of JuMP (v0.11.3) proved unable to build the internal model for input into the solver. We wrote to the developers of JuMP, who informed us that the code for the package had recently been rewritten and that we should try the development version. To do so yourself, run the following commands from Julia:
\begin{lstlisting}
Pkg.checkout("JuMP")
Pkg.update()
\end{lstlisting}
At the same time, we also switched to using the KNITRO solver and disabled the auto-differentiation of the Hessian (to speed up the calculations and out of concern that the AD was introducing errors) by slightly changing the source code of JuMP\footnote{This is easily done by deleting the text ":Hess" in line 341 in the source code at JuMP/src/nlp.jl.}. The results we present for question 4 are based on this last approach (Julia + JuMP development version + KNITRO). The optimization does not converge yet, but, on the other hand, neither JuMP nor KNITRO are returning any errors or warnings and the parameter values seem to evolve nicely with each iteration. 

Separately, we also tried to directly submit the BLP model to the Ipopt solver (without using JuMP to build the internal model). This approach involved manually coding the exact Jacobian of the constraints. Unfortunately, this approach has not worked for us. Ipopt returns a  segmentation error upon trying to call the Jacobian.

\clearpage


\appendix

\section*{Coding Appendix}
\scriptsize	{
\begin{lstlisting}
#####################
##      Setup      ##
#####################

using Ipopt
using KNITRO
using JuMP
using DataFrames
EnableNLPResolve()


#####################
##      Data       ##
#####################

# Load data
product = DataFrames.readtable("dataset_cleaned.csv", separator = ',', header = true);
population = DataFrames.readtable("population_data.csv", separator = ',', header = true);

# Define variables
x = product[:,3:6];
p = product[:,7];
z = product[:,8:13];
s0 = product[:,14];
s = product[:,2];
iv = [x z];
inc = population[:,1];
age = population[:,2];
v = population[:,3:7];

# Store dimensions
K = size(x,2);
L = K+size(z,2);
J = size(x,1);
N = size(v,1);
M = size(v,2);


##########################
##  Simple Logit Model  ##
##########################

# Setup the simple logit model
logit = Model(solver = IpoptSolver(tol = 1e-8, max_iter = 1000, output_file = "logit.txt"));

# Define variables
@defVar(logit, g[1:L]);
@defVar(logit, xi[1:J]);
@defVar(logit, alpha);
@defVar(logit, beta[1:K]);

# We minimize the gmm objective with the identity as the weighting matrix
# subject to the constraints g = sum_j xi_j iv_j and market share equations
@setObjective(logit, Min, sum{g[l]^2,l=1:L});
@addConstraint(
    logit, 
    constr[l=1:L], 
    g[l]==sum{xi[j]*iv[j,l], j=1:J}
);
@addNLConstraint(
    logit, 
    constr[j=1:J], 
    xi[j]==log(s[j])-log(s0[j])+alpha*p[j]-sum{beta[k]*x[j,k],k=1:K}
);

# Solve the model
status = solve(logit);

# Print the results
println("alpha = ", getValue(alpha))
println("beta = ", getValue(beta[1:K]))

# Save results to use in the setup of BLP Model
g_logit=getValue(g);
xi_logit=getValue(xi);
alpha_logit=getValue(alpha);
beta_logit=getValue(beta);


##########################
##      BLP Model       ##
##########################

# Calculate the optimal weighting matrix
iv = convert(Array, iv);
W = inv((1/J)*iv'*Diagonal(diag(xi_logit*xi_logit'))*iv);

# Setup the BLP model
BLP = Model(solver = KnitroSolver(KTR_PARAM_OUTMODE=2, KTR_PARAM_LINSOLVER=5, KTR_PARAM_MAXIT=30));

# Defining variables - set initial values to estimates from the logit model
@defVar(BLP, g[x=1:L], start=(g_logit[x]));
@defVar(BLP, xi[x=1:J], start=(xi_logit[x]));
@defVar(BLP, alpha, start=alpha_logit);
@defVar(BLP, beta[x=1:K], start=beta_logit[x]);

# Defining variables - heterogeneity parameters
@defVar(BLP, piInc[1:K+1]);
@defVar(BLP, piAge[1:K+1]);
@defVar(BLP, sigma[1:K+1]);

# We minimize the gmm objective - using the optimal weighting matrix
# subject to g = sum_j xi_j iv_j and market share equations - 
# Note that where we assign each shock could have minor effect on estimation results
# shock 1 : taste shock to constant
# shock 2 : taste shock to x1
# shock 3 : taste shock to x2
# shock 4 : taste shock to x3
# shock 5 : taste shock to price
@setObjective(BLP,Min,sum{sum{W[i,j]*g[i]*g[j],i=1:L},j=1:L});
@addConstraint(
    BLP, 
    constr[l=1:L], 
    g[l] == sum{xi[j]*iv[j,l],j=1:J}
);

# Trick to increase the sparsity and aid AD
@defVar(BLP, summand[1:N,1:J])
@addConstraint(BLP,
              summand_constr[n=1:N,h=1:J],
              summand[n,h] == -(alpha+piInc[K+1]*inc[n]+piAge[K+1]*age[n]+sigma[K+1]*v[n,K+1])*p[h]
              +sum{(beta[k]+piInc[k]*inc[n]+piAge[k]*age[n]+sigma[k]*v[n,k])*x[h,k],k=1:K}
              +xi[h]
);
@defNLExpr(
    BLP,
    denom[n=1:N],
    sum{exp(summand[n,h]), h=1:J}
);
@addNLConstraint(
    BLP,
    constr[j=1:J], 
    s[j]==(1/N)*sum{exp(summand[n,j])/denom[n],n=1:N}
);

# Solve the model
status = solve(BLP);

# Print the results
println("alpha = ", getValue(alpha))
println("beta = ", getValue(beta[1:K]))
println("piInc = ", getValue(piInc[1:K+1]))
println("piAge = ", getValue(piAge[1:K+1]))
println("sigma = ", getValue(sigma[1:K+1]))


\end{lstlisting}}






\end{document}